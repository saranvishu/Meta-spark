1. Experiment: Design a Unique AR Effect Using Face Reference Assets

Aim

To design a personalized AR effect using face reference assets in Meta Spark 

Studio, enhanced with a background audio loop.

Requirements

• Software: Meta Spark Studio

• Assets:

o Face reference assets (meshes, textures, or premade assets like 

masks)

o A looping audio file (MP3 or WAV format)

• Skills Needed: Basic understanding of Meta Spark Studio’s interface and 

Patch Editor.

• Hardware: Computer with Meta Spark Studio installed, and a camera-

enabled device (for testing).

Procedure

1. Setup the Project:

o Open Meta Spark Studio and create a new face-tracking project.

o Drag a Face Tracker object into the scene.

2. Add Face Reference Assets:

o Import face meshes, textures, or animations into the asset library.

o Attach the face reference assets to the Face Tracker object to 
Add dynamic elements (e.g., animations or color-changing filters) 
triggered by facial expressions or screen taps.
o Use the Patch Editor to create logical connections (e.g., raise 
eyebrows to trigger color changes).
5. Testing and Optimization:
o Preview the effect in real time using a connected camera-enabled 
device.
o Make adjustments to ensure smooth interaction and transitions.
Process and Output
• Process: Users enable the AR effect via their device’s camera, activating 
face-tracking visuals and synchronized background audio.
• Output: A polished AR effect featuring face-based animations, filters, 
and a looping audio experience.
Result
The AR effect achieves high engagement by combining visual customization 
with auditory feedback, delivering an immersive user experience.
2. Experiment: Create and Present a 2D Interactive Effect Using the PEAR 
Framework
Aim
To develop a 2D interactive AR experience in Meta Spark Studio, adhering to 
the PEAR framework (Personalized, Easy to Use, Accessible, Relatable).
Requirements
• Software: Meta Spark Studio
• Assets:
o 2D images or graphic assets (e.g., PNG/JPG files).
o A storyboard or flowchart to outline the user journey.
• Skills Needed: Basic Patch Editor knowledge and design principles 
aligned with PEAR.
Procedure
1. Brainstorm and Conceptualize:
o Choose a relatable theme (e.g., emojis, cultural symbols, or a 
trending topic).
o Ensure the concept is easy to use, accessible across devices, and 
personalized for the target audience.
2. Setup the Project:
o Open a new Meta Spark Studio project and import 2D assets.
o Position assets within the scene, such as attaching them to a 2D 
plane or screen.
3. Design Interaction Flow:
o Use the Patch Editor to set up gestures like screen taps or swipes to 
trigger specific actions (e.g., change colors, display animations).
o Add logic to make the interaction intuitive and responsive.
4. Ensure Accessibility:
o Test the effect on different devices to ensure smooth performance.
o Keep the interface uncluttered and ensure interactive elements are 
large and visible.
5. Create a Storyboard:
o Visualize the user’s journey through sketches or flow diagrams, 
showing how each gesture triggers a corresponding effect.
Process and Output
• Process: The user interacts with 2D graphics through gestures, triggering 
simple and meaningful changes.
• Output: A relatable and engaging 2D AR effect with seamless 
interaction.
Result
The effect aligns with the PEAR framework, offering an interactive, accessible, 
and enjoyable experience.
3. Experiment: Advanced AR Project Using Patch Editor and UI Picker

Aim
To create a customizable AR experience in Meta Spark Studio using the Patch 
Editor and UI Picker for dynamic transitions.
Requirements
• Software: Meta Spark Studio
• Assets:
o Multiple 3D models or textures
o UI Picker template (built-in or custom)
• Skills Needed: Familiarity with Patch Editor and dynamic UI design.
Procedure
1. Project Setup:
o Start a new project and add a Face Tracker or Plane Tracker, 
depending on the effect.
2. Create UI Picker Functionality:
o Import multiple 3D models or textures for the AR scene.
o Add a UI Picker element and customize it with labels for different 
effects (e.g., “Filter 1,” “Filter 2”).
3. Patch Editor Integration:
o Connect the UI Picker options to a Switch node in the Patch Editor.
o Assign each option in the UI Picker to a specific 3D model or 
texture.
4. Dynamic Transitions:
o Use Transition patches to ensure smooth visual changes between 
options.
o Add animations or effects to enhance the overall experience.
5. Testing and Refinement:
o Test the transitions and interactions on multiple devices.
o Ensure smooth operation and debug any logical errors.
Aim
To create an audio-reactive AR effect in Meta Spark Studio using plane tracking 
and real-time sound input.
Requirements
• Software: Meta Spark Studio
• Assets:
o 3D models (e.g., objects, animated characters).
o Audio input from a device microphone.
• Skills Needed: Patch Editor proficiency and understanding of plane 
tracking.
Procedure
1. Setup Plane Tracking:
o Create a new project with a Plane Tracker object to detect surfaces.
2. Add 3D Models:
o Import 3D assets and anchor them to the detected plane.
3. Audio Input Node:
o Add an audio input node in the Patch Editor.
o Connect it to sound-reactive patches (e.g., amplitude analysis).
4. Reactive Animations:
o Use the Patch Editor to link audio data to visual properties, like 
scale, color, or motion of the 3D objects.
5. Testing and Deployment:
o Test the AR effect in different environments to evaluate how sound 
levels influence visual changes.
Process and Output
• Process: 3D objects dynamically respond to real-time audio input, 
creating a visually engaging AR experience.
• Output: A functional and interactive plane-tracked AR effect driven by 
environmental sound.
Result
The AR effect successfully combines plane tracking and audio input to deliver a 
dynamic and immersive experience.
5. Experiment: Create an Interactive 2D Hand-Tracking Effect
Aim
To develop an interactive 2D AR effect using Meta Spark Studio’s hand-
tracking capability to align with specific audience needs.
Requirements
• Software: Meta Spark Studio
• Assets: 2D graphic elements (e.g., shapes, animations).
• Skills Needed: Knowledge of hand tracking and Patch Editor.
Procedure
1. Setup the Project:
o Open Meta Spark Studio and create a hand-tracking project.
o Add the Hand Tracker object to the scene.
2. Design 2D Interactive Elements:
o Import 2D assets into the library (e.g., moving icons, text 
overlays).
o Attach these assets to the Hand Tracker so they follow the user’s 
hand movements
3. Create Interaction Flow:
o Use the Patch Editor to design gesture-based interactions. For 
example: 
▪ Pinch gesture: Change colors or display animations.
▪ Wave gesture: Trigger transitions or other effects.
4. Audience-Specific Needs:
o Tailor the interaction to your target audience (e.g., educational 
gestures for children or functional gestures for fitness tracking).
5. Testing and Refinements:
o Test the hand-tracking performance across various devices and 
lighting conditions.
o Optimize the design for smooth responsiveness.
Process and Output
• Process: The user’s hand movement triggers dynamic 2D effects, 
creating an interactive experience.
• Output: A 2D AR effect that engages the audience through responsive 
hand-tracking interactions.
Result
The AR effect successfully responds to user gestures, offering an immersive 
experience that aligns with audience needs.
6. Experiment: Develop a Dynamic World AR Effect
Aim
To create a World AR experience using plane tracking, placing 3D models in 
real-world environments with looping sound effects.
Requirements
• Software: Meta Spark Studio
• Assets:
o 3D models (e.g., characters, furniture, or objects).
o Background sound file (loopable)
• Skills Needed: Knowledge of plane tracking and sound integration.
Procedure
1. Setup Plane Tracking:
o Start a new World AR project and add a Plane Tracker to detect 
flat surfaces.
2. Import and Anchor 3D Models:
o Import 3D objects and position them in the scene relative to the 
Plane Tracker.
3. Add Dynamic Audio:
o Import a looping audio file and use the Patch Editor to enable 
continuous playback.
o Link the sound to specific animations or object actions (e.g., 
movement of a 3D character).
4. Create Interaction and Transitions:
o Allow users to interact with the objects (e.g., tap to move or 
resize).
o Use the Patch Editor to create transitions between object states 
(e.g., idle to animated).
5. Test in Real-World Environments:
o Evaluate performance across various surfaces, lighting conditions, 
and device types.
Process and Output
• Process: The user places and interacts with 3D objects in their physical 
environment, accompanied by dynamic sound effects.
• Output: A World AR experience with interactive 3D models and looping 
audio.
Result
The AR effect enhances real-world spaces with interactive 3D visuals and 
auditory feedback.
7. Experiment: Create a Face-Tracking AR Effect with Audio-Reactive 
Masks
Aim
To develop an AR effect that uses face tracking and audio inputs to trigger 
dynamic mask changes.
Requirements
• Software: Meta Spark Studio
• Assets:
o 3D face masks or elements (e.g., glasses, hats, or animated 
overlays).
o Audio-reactive patches.
• Skills Needed: Patch Editor experience and face-tracking proficiency.
Procedure
1. Setup Face Tracking:
o Create a new face-tracking project and add a Face Tracker object.
2. Add Face Masks:
o Import multiple 3D masks or overlay elements into the project.
o Attach these masks to the Face Tracker.
3. Audio Input and Reaction:
o Add an audio input node in the Patch Editor to capture sound from 
the microphone.
o Connect the audio input to control mask properties, such as 
opacity, color, or animations.
4. Customize Visual Effects:
o Use audio-reactive patches (e.g., amplitude or frequency analyzers) 
to trigger mask transitions based on sound intensity.
5. Testing and Optimization:
o Test the audio responsiveness and face-tracking accuracy under 
varying conditions.
Process and Output
8. Experiment: Design a Dynamic Face Effect with Text Integration
Aim
To create a face-tracking AR effect that integrates text dynamically with visual 
effects.
Requirements
• Software: Meta Spark Studio
• Assets:
o Text assets or fonts.
o Face reference models.
• Skills Needed: Basic face-tracking and text animation knowledge.
Procedure
1. Setup Face Tracking:
o Start a new face-tracking project and add a Face Tracker to the 
scene.
2. Import Text Assets:
o Add text elements to the scene, such as floating captions, quotes, or 
labels.
3. Dynamic Text Animation:
o Use the Patch Editor to animate text properties (e.g., size, color, 
position).
o Trigger text changes based on facial gestures (e.g., blinking to 
display new text).

4. Incorporate Additional Visuals:
o Add dynamic overlays or effects that enhance the visual appeal of 
the text.
5. Testing:
o Test the interaction across multiple devices to ensure smooth 
tracking and text display.
Process and Output
• Process: Dynamic text changes based on face movements or gestures, 
offering a personalized AR interaction.
• Output: A face-tracking AR effect with animated, interactive text.
Result
The AR effect provides engaging and customizable text overlays with face-
tracking accuracy.
9. Experiment: Create a 3D Object Tracking AR Effect
Aim
To design a 3D AR effect where object tracking responds to user interactions, 
incorporating audio for an enhanced experience.
Requirements
• Software: Meta Spark Studio
• Assets:
o 3D models.
o Background audio file.
• Skills Needed: Object tracking and interaction design knowledge.
Procedure
1. Setup Object Tracking:
o Create a new object-tracking project in Meta Spark Studio.
o Add tracking anchors to detect specific objects (e.g., books or 
cards).

2. Place 3D Models:
o Import 3D assets and position them relative to the tracked object.
3. Audio Integration:
o Add background audio and enable playback when the tracked 
object is detected.
4. User Interaction:
o Use the Patch Editor to allow user interactions, such as tapping to 
animate the 3D model or change its properties.
5. Testing:
o Test object tracking accuracy and interaction responsiveness in 
various scenarios.
Process and Output
• Process: Users interact with 3D models anchored to real-world objects, 
with background audio enhancing the experience.
• Output: An interactive object-tracking AR effect with dynamic visuals 
and sound.
Result
The AR effect successfully tracks objects and responds to user interactions with 
immersive visuals and sound.
10. Experiment: Image-Tracking AR Effect
Aim
To create an image-tracking AR effect that displays animated 3D objects with a 
looping audio component.
Requirements
• Software: Meta Spark Studio
• Assets:
o Image targets (e.g., posters or logos).
o Animated 3D models.
o Looping audio file.
Skills Needed: Image tracking and 3D animation knowledge.
Procedure
1. Setup Image Tracking:
o Start a new image-tracking project and import the target image.
o Add an Image Tracker object and configure it to recognize the 
target.
2. Place Animated 3D Objects:
o Import 3D models and position them relative to the image tracker.
3. Audio Integration:
o Add looping audio to enhance the effect and synchronize it with 
the 3D animation.
4. Testing:
o Test image detection accuracy and animation responsiveness across 
different lighting conditions.
Process and Output
• Process: The AR effect triggers when the camera detects the image, 
displaying animated 3D objects with synchronized audio.
• Output: An image-tracking AR effect with interactive animations and 
sound.
Result
The AR effect provides a seamless and engaging experience by integrating 
image tracking, 3D visuals, and audio.





